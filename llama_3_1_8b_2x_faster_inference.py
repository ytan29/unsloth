# -*- coding: utf-8 -*-
"""Llama-3.1 8b 2x faster inference

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aqlNQi7MMJbynFDyOQteD2t0yVfjb9Zh

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Join Discord if you need help + support us if you can!
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install unsloth
# # Get latest Unsloth
# !pip install --upgrade --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

"""If you want to finetune Llama-3 2x faster and use 70% less VRAM, go to our [finetuning notebook](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)!"""

### Set Proxy ###
# import os
# os.environ['HTTP_PROXY'] = "http://proxy-dmz.intel.com:912"
# os.environ['HTTPS_PROXY'] = "http://proxy-dmz.intel.com:912"

from unsloth import FastLanguageModel
import torch
import timeit
import time
# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/gemma-7b-it-bnb-4bit",
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
#    model_name = "/home/ct/unsloth-test/Llama-3.2-1B-Instruct",
#    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct",
#    model_name = "meta-llama/Meta-Llama-3-8B-Instruct",
#    model_name = "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
    model_name = "meta-llama/Llama-3.2-1B-Instruct",
#    model_name = "meta-llama/Llama-3.2-3B-Instruct",
    max_seq_length = 8192,
    load_in_4bit = False, # KCT : original value = True
#    local_files_only = True,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

from transformers import TextStreamer
from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

"""Change the "value" part to call the model!

Unsloth makes inference natively 2x faster!! No need to change or do anything!
"""
model = model.to("xpu")


# messages = [
#                                # EDIT HERE!
#     {"from": "human", "value": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
# ]
# inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = "pt").to("xpu")

# text_streamer = TextStreamer(tokenizer)

# start_time = time.time()
# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
# end_time = time.time()
# print(f"Inference time: {end_time - start_time} seconds")


messages = [
    {"from": "human", "value": "Describe the tallest tower in the world."},
]
inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = "pt").to("xpu")

text_streamer = TextStreamer(tokenizer)

start_time = time.time()
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
end_time = time.time()
print(f"Inference time: {end_time - start_time} seconds")


# messages = [
#     {"from": "human", "value": "What is Unsloth?"},
# ]
# inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = "pt").to("xpu")

# text_streamer = TextStreamer(tokenizer)

# start_time = time.time()
# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
# end_time = time.time()
# print(f"Inference time: {end_time - start_time} seconds")


# messages = [
#     {
#         "role": "system",
#         "content": "You are a skilled Python developer specializing in database management and optimization.",
#     },
#     {
#         "role": "user",
#         "content": "I'm experiencing a sorting issue in my database. Could you please provide Python code to help resolve this problem?",
#     },
# ]

# inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = "pt").to("xpu")
# text_streamer = TextStreamer(tokenizer)
# start_time = time.time()
# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
# end_time = time.time()
# print(f"Inference time: {end_time - start_time} seconds")

# Alternative (for reference)
# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
# inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to("xpu")
# text_streamer = TextStreamer(tokenizer)
# start_time = time.time()
# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)
# end_time = time.time()
# print(f"Inference time: {end_time - start_time} seconds")




### Profiling with timeit ###
# warm up
# for _ in range(3):
#     _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)

# def run_inference():
#     #torch.xpu.synchronize()
#     _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
#     #torch.xpu.synchronize()

# iter = 5
# inference_time = timeit.timeit(run_inference, number=iter)
# print(f"Average inference time over {iter} runs: {inference_time / iter:.6f} seconds")
### Profiling with timeit ###


### Profiling with time ###
# warm up
# for _ in range(3):
#     _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)

# iter = 5
# start_time = time.time()
# for _ in range(iter):
#     #torch.xpu.synchronize()
#     _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
#     #torch.xpu.synchronize()
# end_time = time.time()

# print(f"Inference time: {(end_time - start_time)/iter:.6f} seconds")
### Profiling with time ###


### Profiling with torch profiler ###
### https://pytorch.org/docs/stable/autograd.html#profiler ###
# with torch.autograd.profiler.profile(use_device="xpu", use_kineto=True) as prof:
#     # do what you want to profile here after the `with` statement with proper indent
#     _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
# print(prof.key_averages().table(sort_by="self_xpu_time_total"))
### Profiling with torch profiler ###



"""And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

If you want to finetune Llama-3 2x faster and use 70% less VRAM, go to our [finetuning notebook](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)!

Some other links:
1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)
3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)
5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ðŸ¤— HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!
7. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
9. Gemma 6 trillion tokens is 2.5x faster! [free Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)

<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> Support our work if you can! Thanks!
</div>
"""